# ğŸ›’ Olist E-Commerce Analysis

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/)
[![Marimo](https://img.shields.io/badge/Marimo-Interactive-orange.svg)](https://marimo.io/)
[![DuckDB](https://img.shields.io/badge/DuckDB-SQL-yellow.svg)](https://duckdb.org/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

Analyse descriptive et prÃ©dictive du dataset **Olist E-Commerce** (BrÃ©sil, 2016-2018).

## ğŸ“‹ Description

Ce projet rÃ©alise une analyse approfondie des donnÃ©es de commandes de la plateforme e-commerce brÃ©silienne **Olist** sur la pÃ©riode 2016-2018. L'objectif est de :

1. âœ… **Analyser** les donnÃ©es de maniÃ¨re descriptive avec visualisations interactives
2. âœ… **Nettoyer** et prÃ©parer les datasets pour exploitation
3. âœ… **Structurer** les donnÃ©es dans une base SQL avec schÃ©ma optimisÃ©
4. ğŸ”® **PrÃ©dire** (optionnel) les tendances de commandes futures

## ğŸ¯ Objectifs

### Obligatoires (PrioritÃ© 1)
- [x] Analyse descriptive approfondie des 9 fichiers CSV
- [x] Script Python de nettoyage de donnÃ©es (POO)
- [x] Notebooks Marimo interactifs pour visualisations
- [x] Base de donnÃ©es SQL avec schÃ©ma dÃ©duit
- [x] Documentation complÃ¨te

### Optionnels (PrioritÃ© 2)
- [ ] Analyse prÃ©dictive des commandes
- [ ] Comparaison prÃ©dictions vs donnÃ©es rÃ©elles
- [ ] Dashboard interactif complet

## ğŸ“Š Dataset

- **Source** : [Kaggle - Olist Brazilian E-Commerce](https://www.kaggle.com/olistbr/brazilian-ecommerce)
- **Contenu** : 9 fichiers CSV avec commandes, clients, produits, paiements, reviews...
- **PÃ©riode** : 2016-2018
- **Taille** : ~100k commandes

## ğŸš€ DÃ©marrage Rapide

### PrÃ©requis

- Python 3.8+
- `uv` (gestionnaire de paquets rapide)
- Compte Kaggle (pour tÃ©lÃ©chargement automatique)

### Installation

```powershell
# 1. Cloner le projet
git clone <URL>
cd olist-analysis

# 2. CrÃ©er environnement virtuel avec uv
uv venv
.venv\Scripts\activate  # PowerShell

# 3. Installer les dÃ©pendances
uv pip install -e .

# 4. Configuration Kaggle (optionnel, sinon tÃ©lÃ©chargement manuel)
# Placer kaggle.json dans ~/.kaggle/ ou C:\Users\<user>\.kaggle\
```

### Utilisation

```powershell
# TÃ©lÃ©charger les donnÃ©es depuis Kaggle
uv run python scripts/download_data.py

# Valider la qualitÃ© des donnÃ©es avec Pandera
uv run python scripts/validate_data.py

# Nettoyer et prÃ©parer les donnÃ©es
uv run python scripts/process_data.py

# Charger dans la base de donnÃ©es DuckDB
uv run python scripts/load_to_db.py

# Lancer les notebooks interactifs Marimo
marimo edit notebooks/01_exploration.py
marimo edit notebooks/02_descriptive_analysis.py
```

## ğŸ“ Structure du Projet

```
olist-analysis/
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ copilot-instructions.md    # Instructions pour GitHub Copilot
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                        # DonnÃ©es brutes Kaggle
â”‚   â”œâ”€â”€ processed/                  # DonnÃ©es nettoyÃ©es
â”‚   â””â”€â”€ olist.duckdb               # Base de donnÃ©es
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ loader.py              # Chargement Kaggle
â”‚   â”‚   â”œâ”€â”€ cleaner.py             # Nettoyage (POO)
â”‚   â”‚   â””â”€â”€ validator.py           # Validation qualitÃ©
â”‚   â”œâ”€â”€ database/
â”‚   â”‚   â”œâ”€â”€ connection.py          # Connexion DB
â”‚   â”‚   â”œâ”€â”€ writer.py              # Ã‰criture SQL
â”‚   â”‚   â””â”€â”€ schema.py              # SchÃ©ma tables
â”‚   â”œâ”€â”€ analysis/
â”‚   â”‚   â”œâ”€â”€ descriptive.py         # Stats descriptives
â”‚   â”‚   â””â”€â”€ preprocessing.py       # Transformations
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ config.py              # Configuration
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_exploration.py          # Marimo: Exploration
â”‚   â”œâ”€â”€ 02_descriptive_analysis.py # Marimo: Analyses
â”‚   â””â”€â”€ 03_data_quality.py         # Marimo: QualitÃ©
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ download_data.py           # Script tÃ©lÃ©chargement
â”‚   â”œâ”€â”€ process_data.py            # Script nettoyage
â”‚   â””â”€â”€ load_to_db.py              # Script chargement DB
â”œâ”€â”€ tests/                          # Tests unitaires
â”œâ”€â”€ .gitignore
â”œâ”€â”€ pyproject.toml                  # DÃ©pendances
â””â”€â”€ README.md
```

## ğŸ“– Documentation

### Modules Principaux

#### 1. Data Loading (`src/data/loader.py`)

```python
from src.data.loader import OlistDataLoader

loader = OlistDataLoader()
dataframes = loader.load_all()
print(loader.summary())
```

#### 2. Data Cleaning (`src/data/cleaner.py`)

```python
from src.data.cleaner import DataCleaner

cleaner = DataCleaner(df, name="orders")
cleaned_df = (
    cleaner
    .remove_duplicates()
    .handle_missing_values(strategy='drop')
    .convert_dtypes({'order_date': 'datetime64'})
    .get_cleaned_data()
)
cleaner.print_summary()
```

#### 3. Database Integration (`src/database/`)

```python
from src.database.connection import DatabaseConnection
from src.database.writer import DatabaseWriter

db_conn = DatabaseConnection(db_type='duckdb')
with db_conn:
    writer = DatabaseWriter(db_conn)
    writer.write_multiple(dataframes)
```

#### 4. Data Validation avec Pandera (`src/data/schemas.py`)

```python
from src.data.validator import DataValidator
from src.data.schemas import get_schema

# Validation avec schÃ©ma prÃ©dÃ©fini
validator = DataValidator(orders_df, name="orders")
schema = get_schema('olist_orders_dataset')
is_valid = validator.validate_with_schema(schema)

# Ou utiliser le script de validation complet
# uv run python scripts/validate_data.py
```

## ğŸ› ï¸ Technologies

| Outil          | Usage                              |
|----------------|------------------------------------|
| **Python**     | Langage principal                  |
| **uv**         | Gestion de dÃ©pendances rapide      |
| **Pandas**     | Manipulation de donnÃ©es            |
| **Pandera**    | Validation de schÃ©mas DataFrame    |
| **Marimo**     | Notebooks interactifs              |
| **DuckDB**     | Base de donnÃ©es SQL embarquÃ©e      |
| **kagglehub**  | TÃ©lÃ©chargement dataset Kaggle      |
| **Plotly**     | Visualisations interactives        |
| **SQLAlchemy** | ORM pour schÃ©ma SQL                |

## ğŸ“ˆ Exemples d'Analyses

Les notebooks Marimo fournissent :

- ğŸ“Š Statistiques descriptives par table
- ğŸ“‰ Ã‰volution temporelle des commandes
- ğŸ—ºï¸ Distribution gÃ©ographique des clients
- ğŸ’° Analyse des prix et paiements
- â­ Analyse des reviews et satisfaction
- ğŸ”— Relations entre tables (clÃ©s Ã©trangÃ¨res)

## ğŸ¤ Conventions de Commits

Format : `type(scope): description`

**Types** :
- `feat`: Nouvelle fonctionnalitÃ©
- `fix`: Correction de bug
- `docs`: Documentation
- `refactor`: Refactorisation
- `test`: Tests
- `chore`: Maintenance

**Exemples** :
```
feat(data): add Kaggle data loader
feat(analysis): add descriptive statistics
docs(readme): add installation instructions
refactor(cleaner): apply OOP pattern
```

## ğŸ•’ Planning

| Phase | DurÃ©e | TÃ¢ches |
|-------|-------|--------|
| 1. Setup | 30 min | Installation, tÃ©lÃ©chargement donnÃ©es |
| 2. Exploration | 1h | Notebooks Marimo d'exploration |
| 3. Analyse | 2h | Analyses descriptives complÃ¨tes |
| 4. Nettoyage | 1h30 | Classes POO de nettoyage |
| 5. SQL | 1h | SchÃ©ma et chargement en base |
| 6. Docs | 30 min | README, docstrings, commits |

**Temps total estimÃ©** : ~6h30

## ğŸ“ License

MIT License - Voir [LICENSE](LICENSE) pour plus de dÃ©tails.

## ğŸ‘¤ Auteur

Projet rÃ©alisÃ© dans le cadre de la formation Data Engineering Simplon.

---

**DerniÃ¨re mise Ã  jour** : 2026-02-09
